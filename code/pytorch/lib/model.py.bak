import os, time
import torch
import torch.optim as optim
from torch.optim.lr_scheduler import ReduceLROnPlateau
import torch.backends.cudnn as cudnn
import numpy as np


from .arch import Architecture
try:
    from .dice import DiceLoss, DiceCoefficient
except Exception:
    DiceLoss = None
    DiceCoefficient = None

class Model(object):

    def __init__(self, labels, input_shape, load_model_path='', usegpu=True):

        self.labels = labels
        self.n_classes = len(labels)
        self.load_model_path = load_model_path
        self.usegpu = usegpu
        self.input_shape = input_shape
        self.model = Architecture(self.n_classes, self.input_shape, usegpu=self.usegpu)

        self.__load_weights()

        if self.usegpu:
            cudnn.benchmark = True
            self.model.cuda()
            #self.model = torch.nn.DataParallel(self.model, device_ids=range(self.ngpus))

        print(self.model)

    def __load_weights(self):
    if self.load_model_path == '':
        return

       assert os.path.isfile(self.load_model_path), \
        'Model : {} does not exists!'.format(self.load_model_path)

        # Always load the checkpoint onto CPU so it works on machines without CUDA
        state = torch.load(self.load_model_path, map_location="cpu")

        # Load non-strict to ignore missing keys like conv53.p_logit / conv12d.p_logit
        missing, unexpected = self.model.load_state_dict(state, strict=False)
        try:
        print("Loaded with strict=False. Missing:", missing, "Unexpected:", unexpected)
        except Exception:
        pass

    def __define_variable(self, tensor, volatile=False):
        if getattr(self, "usegpu", False) and torch.cuda.is_available():
            return tensor.cuda(non_blocking=True)
    return tensor

    def __define_input_variables(self, features, labels, volatile=False):
        features_var = self.__define_variable(features, volatile=volatile)
        labels_var = self.__define_variable(labels, volatile=volatile)

        return features_var, labels_var

    def __define_criterion(self, class_weights, optimize_bg=False, criterion='CE'):
        assert criterion in ['CE', 'Dice', 'Multi', None]

        smooth = 1.0

        self.criterion_dice_coeff = DiceCoefficient(smooth=smooth)

        if type(criterion) == type(None):
            return

        if type(class_weights) != type(None):
            class_weights = self.__define_variable(torch.FloatTensor(class_weights))
            if criterion == 'CE':
                self.criterion_ce = torch.nn.CrossEntropyLoss(class_weights)
            elif criterion == 'Dice':
                self.criterion_dice = DiceLoss(optimize_bg=optimize_bg, weight=class_weights, smooth=smooth)
            elif criterion == 'Multi':
                self.criterion_ce = torch.nn.CrossEntropyLoss(class_weights)
                self.criterion_dice = DiceLoss(optimize_bg=optimize_bg, weight=class_weights, smooth=smooth)
        else:
            if criterion == 'CE':
                self.criterion_ce = torch.nn.CrossEntropyLoss()
            elif criterion == 'Dice':
                self.criterion_dice = DiceLoss(optimize_bg=optimize_bg, smooth=smooth)
            elif criterion == 'Multi':
                self.criterion_ce = torch.nn.CrossEntropyLoss()
                self.criterion_dice = DiceLoss(optimize_bg=optimize_bg, smooth=smooth)

        if self.usegpu:
            if criterion == 'CE':
                self.criterion_ce = self.criterion_ce.cuda()
            elif criterion == 'Dice':
                self.criterion_dice = self.criterion_dice.cuda()
            elif criterion == 'Multi':
                self.criterion_ce = self.criterion_ce.cuda()
                self.criterion_dice = self.criterion_dice.cuda()

    def __define_optimizer(self, learning_rate, weight_decay, lr_drop_factor, lr_drop_patience, optimizer='Adam'):
        assert optimizer in ['RMSprop', 'Adam', 'Adadelta', 'SGD']

        parameters = filter(lambda p: p.requires_grad, self.model.parameters())

        if optimizer == 'RMSprop':
            self.optimizer = optim.RMSprop(parameters, lr=learning_rate, weight_decay=weight_decay)
        elif optimizer == 'Adadelta':
            self.optimizer = optim.Adadelta(parameters, lr=learning_rate, weight_decay=weight_decay)
        elif optimizer == 'Adam':
            self.optimizer = optim.Adam(parameters, lr=learning_rate, weight_decay=weight_decay)
        elif optimizer == 'SGD':
            self.optimizer = optim.SGD(parameters, lr=learning_rate, momentum=0.9, weight_decay=weight_decay)

        self.lr_scheduler = ReduceLROnPlateau(self.optimizer, mode='max', factor=lr_drop_factor, patience=lr_drop_patience, verbose=True)

    @staticmethod
    def __get_loss_averager():
        return averager()

    def __minibatch(self, train_test_iter, clip_grad_norm, criterion_type, train_cnn=True, mode='training'):
        assert mode in ['training', 'test'], 'Mode must be either "training" or "test"'

        if mode == 'training':
            for param in self.model.parameters():
                param.requires_grad = True
            if not train_cnn:
                for param in self.model.cnn.parameters():
                    param.requires_grad = False
            self.model.train()
        else:
            for param in self.model.parameters():
                param.requires_grad = False
            self.model.eval()

        cpu_images, cpu_annotations = next(train_test_iter)
        cpu_images = cpu_images.contiguous()
        cpu_annotations = cpu_annotations.contiguous()

        if self.usegpu:
            gpu_images = cpu_images.cuda(non_blocking=True)
            gpu_annotations = cpu_annotations.cud(non_blocking=True)
        else:
            gpu_images = cpu_images
            gpu_annotations = cpu_annotations

        if mode == 'training':
            gpu_images, gpu_annotations = self.__define_input_variables(gpu_images, gpu_annotations)
        else:
            gpu_images, gpu_annotations = self.__define_input_variables(gpu_images, gpu_annotations, volatile=True)

        predictions = self.model(gpu_images)

        if mode == 'training':
            if criterion_type == 'CE':
                _, gpu_annotations_criterion_ce = gpu_annotations.max(3)
                cost = self.criterion_ce(predictions.permute(0, 2, 3, 1).contiguous().view(-1, self.n_classes),
                                         gpu_annotations_criterion_ce.view(-1))
            elif criterion_type == 'Dice':
                gpu_annotations_criterion_dice = gpu_annotations.permute(0, 3, 1, 2).contiguous()
                cost = self.criterion_dice(predictions, gpu_annotations_criterion_dice)
            elif criterion_type == 'Multi':
                _, gpu_annotations_criterion_ce = gpu_annotations.max(3)
                cost_ce = self.criterion_ce(predictions.permute(0, 2, 3, 1).contiguous().view(-1, self.n_classes),
                                            gpu_annotations_criterion_ce.view(-1))
                gpu_annotations_criterion_dice = gpu_annotations.permute(0, 3, 1, 2).contiguous()
                cost_dice = self.criterion_dice(predictions, gpu_annotations_criterion_dice)
                cost = cost_ce + cost_dice
            cost += self.model.regularisation_loss()
        else:
            gpu_annotations_criterion_dice = gpu_annotations.permute(0, 3, 1, 2).contiguous()
            cost = self.criterion_dice_coeff(predictions, gpu_annotations_criterion_dice)
        
        if mode == 'training':
            self.model.zero_grad()
            cost.backward()
            if clip_grad_norm != 0:
                torch.nn.utils.clip_grad_norm(self.model.parameters(), clip_grad_norm)
            self.optimizer.step()

        return cost, predictions, cpu_annotations

    def __test(self, test_loader):

        print('***** Testing *****')

        n_minibatches = len(test_loader)

        test_iter = iter(test_loader)
        n_correct, n_total = 0.0, 0.0
        dice_coefficients = []

        for minibatch_index in range(n_minibatches):
            dice_coeff, predictions, cpu_annotations = self.__minibatch(test_iter, 0.0, None, False, mode='test')

            _, predictions = predictions.max(1)
            _, cpu_annotations = cpu_annotations.max(3)
            dice_coeff = dice_coeff.data

            n_correct += torch.sum(predictions.data.cpu() == cpu_annotations)
            n_total += predictions.numel()

            dice_coefficients.extend(dice_coeff)

        dice_coefficients = torch.stack(dice_coefficients, dim=0).mean(dim=0)

        accuracy = n_correct / n_total

        dice_coefficients_str = ''

        for i, coeff in enumerate(dice_coefficients):
            label_name = self.labels[self.labels[:, 0].astype('int') == i][0, 1]
            if i % 3 == 0:
                dice_coefficients_str += '\n'
            dice_coefficients_str += '| {:25} : {:.5f} |'.format(label_name, coeff)

        print('Accuracy: {}'.format(accuracy))
        print('Dice Coefficients:')
        print(dice_coefficients_str)

        mean_dice_coeff = dice_coefficients[1:].mean() # Discard bg class when calculating mean
        print('Mean Dice: {}'.format(mean_dice_coeff))

        return accuracy, mean_dice_coeff

    def fit(self, criterion_type, learning_rate, weight_decay, clip_grad_norm, lr_drop_factor, lr_drop_patience, optimize_bg, optimizer,
            train_cnn, n_epochs, class_weights, train_loader, test_loader, model_save_path):

        assert criterion_type in ['CE', 'Dice', 'Multi']

        training_log_file = open(os.path.join(model_save_path, 'training.log'), 'w')
        validation_log_file = open(os.path.join(model_save_path, 'validation.log'), 'w')

        training_log_file.write('Epoch,Loss,Accuracy\n')
        validation_log_file.write('Epoch,DiceCoefficient,Accuracy\n')

        train_loss_averager = Model.__get_loss_averager()

        self.__define_criterion(class_weights, optimize_bg=optimize_bg, criterion=criterion_type)
        self.__define_optimizer(learning_rate, weight_decay, lr_drop_factor, lr_drop_patience, optimizer=optimizer)

        self.__test(test_loader)

        best_val_dice_coeff, best_val_acc = 0.0, 0.0
        for epoch in range(n_epochs):
            epoch_start = time.time()

            train_iter = iter(train_loader)
            n_minibatches = len(train_loader)

            minibatch_index = 0
            train_n_correct, train_n_total = 0.0, 0.0
            while minibatch_index < n_minibatches:
                minibatch_cost, minibatch_predictions, minibatch_cpu_annotations = self.__minibatch(train_iter, clip_grad_norm, criterion_type,
                                                                                                    train_cnn=train_cnn, mode='training')

                _, minibatch_predictions = minibatch_predictions.max(1)
                _, minibatch_cpu_annotations = minibatch_cpu_annotations.max(3)
                train_n_correct += torch.sum(minibatch_predictions.data.cpu() == minibatch_cpu_annotations)
                train_n_total += minibatch_predictions.numel()

                train_loss_averager.add(minibatch_cost)
                minibatch_index += 1

            train_accuracy = train_n_correct / train_n_total
            train_loss = train_loss_averager.val()

            epoch_end = time.time()
            epoch_duration = epoch_end - epoch_start

            print('[{}] [{}/{}] Loss : {} - Accuracy : {}'.format(epoch_duration, epoch, n_epochs, train_loss,
                                                                  train_accuracy))

            val_accuracy, val_dice_coeff = self.__test(test_loader)

            self.lr_scheduler.step(val_dice_coeff)

            is_best_model_dice_coeff = val_dice_coeff >= best_val_dice_coeff
            is_best_model_acc = val_accuracy >= best_val_acc
            is_best_model = is_best_model_dice_coeff or is_best_model_acc

            if is_best_model:
                if is_best_model_dice_coeff:
                    best_val_dice_coeff = val_dice_coeff
                if is_best_model_acc:
                    best_val_acc = val_accuracy
                torch.save(self.model.state_dict(), os.path.join(model_save_path, 'model_{}_{}_{}.pth'.format(epoch, val_dice_coeff, val_accuracy)))

            training_log_file.write('{},{},{}\n'.format(epoch, train_loss, train_accuracy))
            validation_log_file.write('{},{},{}\n'.format(epoch, val_dice_coeff, val_accuracy))
            training_log_file.flush()
            validation_log_file.flush()

            train_loss_averager.reset()
            train_n_correct, train_n_total = 0.0, 0.0

        training_log_file.close()
        validation_log_file.close()

    def test(self, class_weights, test_loader):

        self.__define_criterion(class_weights, optimize_bg=False, criterion=None)
        test_accuracy, test_dice_coeff = self.__test(test_loader)

        return test_accuracy, test_dice_coeff

    def predict(self, images):

        assert len(images.size()) == 4 #b, c, h, w

        for param in self.model.parameters():
            param.requires_grad = False
        self.model.eval()

        images = images.contiguous()
        if self.usegpu:
            images = images.cuda(non_blocking=True)

        images = self.__define_variable(images, volatile=True)

        predictions = self.model(images)

        return predictions

class averager(object):
    """Compute average for `torch.Variable` and `torch.Tensor`."""

    def __init__(self):
        self.reset()

    def add(self, v):
        if isinstance(v, Variable):
            count = v.data.numel()
            v = v.data.sum()
        elif isinstance(v, torch.Tensor):
            count = v.numel()
            v = v.sum()

        self.n_count += count
        self.sum += v

    def reset(self):
        self.n_count = 0
        self.sum = 0

    def val(self):
        res = 0
        if self.n_count != 0:
            res = self.sum / float(self.n_count)
        return res

import torch
import numpy as np
from PIL import Image

class Prediction(object):
    """
    Minimal prediction wrapper compatible with pred_folder.py.
    Uses the already-loaded self.model inside Model.
    """
    def __init__(self, img_h, img_w, mean, std, model):
        # img_h/img_w are the network input size from ModelSettings
        self.img_h = int(img_h)
        self.img_w = int(img_w)
        self.mean = np.array(mean, dtype=np.float32).reshape(1, 1, 3)
        self.std  = np.array(std,  dtype=np.float32).reshape(1, 1, 3)

        # 'model' here is an instance of lib.model.Model
        self.net = model.model.eval()
        self.usegpu = getattr(model, "usegpu", False) and torch.cuda.is_available()

    def _preprocess(self, pil_img):
        # resize to network input, normalize to CHW
        img = pil_img.convert("RGB").resize((self.img_w, self.img_h), Image.BILINEAR)
        arr = np.asarray(img).astype(np.float32) / 255.0
        arr = (arr - self.mean) / self.std
        arr = np.transpose(arr, (2, 0, 1))  # HWC -> CHW
        t = torch.from_numpy(arr).unsqueeze(0)  # 1x3xHxW
        if self.usegpu:
            t = t.cuda(non_blocking=True)
        return t

    @torch.no_grad()
    def predict(self, image_path, n_samples=1):
        """
        Returns (original_pil, pred_array, vis_var_dummy, variance_dummy)
        pred_array is HxW uint8 with class ids (resized back to original image size).
        """
        pil = Image.open(image_path).convert("RGB")
        orig_w, orig_h = pil.size

        inp = self._preprocess(pil)
        logits = self.net(inp)                    # shape: 1 x C x H x W
        pred_small = torch.argmax(logits, dim=1)  # 1 x H x W
        pred_small = pred_small.squeeze(0).cpu().numpy().astype(np.uint8)

        # resize to original image size for saving/compatibility
        pred = Image.fromarray(pred_small, mode="L").resize((orig_w, orig_h), Image.NEAREST)
        pred_np = np.array(pred, dtype=np.uint8)

        # we don’t compute uncertainty here; return dummies to keep API compatible
        vis_var = None
        variance = None
        return pil, pred_np, vis_var, variance
